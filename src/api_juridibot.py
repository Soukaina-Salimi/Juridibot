from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
import os
import faiss
import pandas as pd
from pathlib import Path
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
from openai import OpenAI

# =====================================================
# üîπ Chargement des variables d'environnement
# =====================================================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise RuntimeError("‚ùå OPENAI_API_KEY non trouv√©e. Cr√©e un fichier .env avec ta cl√© OpenAI.")

client = OpenAI(api_key=OPENAI_API_KEY)

# =====================================================
# üîπ Configuration du projet
# =====================================================
BASE = Path(__file__).resolve().parents[1]
INDEX_FILE = BASE / "data/cleaned_chunks/faiss_index.bin"
META_FILE = BASE / "data/cleaned_chunks/chunks_meta.parquet"

EMBED_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"
TOP_K = 5
DISTANCE_THRESHOLD = 7.5
MAX_CONTEXT_CHARS = 2000

# =====================================================
# üîπ Prompts d'instructions
# =====================================================
SYSTEM_PROMPT = (
    "Tu es **JuridiBot**, un assistant juridique marocain. "
    "Tu r√©ponds uniquement sur la base des textes juridiques fournis "
    "(Code du travail, Code de la famille, droit p√©nal, etc.). "
    "Si la question n‚Äôa aucun rapport avec ces documents, tu dois r√©pondre : "
    "'Je ne peux pas r√©pondre √† cette question car elle ne figure pas dans ma base de connaissances juridiques.' "
    "Toujours citer les sources utilis√©es entre crochets √† la fin."
)

INSTRUCTION_PROMPT = (
    "Utilise exclusivement les extraits suivants pour formuler ta r√©ponse. "
    "Ne cr√©e ni n'invente d'informations ext√©rieures √† ces textes."
)

# =====================================================
# üîπ Chargement du mod√®le et des donn√©es
# =====================================================
print("Chargement de l‚Äôindex FAISS et des m√©tadonn√©es...")
index = faiss.read_index(str(INDEX_FILE))
df_meta = pd.read_parquet(META_FILE)
print(f"‚úÖ Index charg√© ({index.ntotal} vecteurs).")

embedder = SentenceTransformer(EMBED_MODEL)

# =====================================================
# üîπ Fonctions principales
# =====================================================
def retrieve(query: str, top_k: int = TOP_K):
    """Recherche des passages similaires"""
    q_vec = embedder.encode([query], convert_to_numpy=True)
    D, I = index.search(q_vec, top_k)
    results = []
    for idx, dist in zip(I[0], D[0]):
        row = df_meta.iloc[idx]
        results.append({
            "chunk_id": row["chunk_id"],
            "source": row.get("source", "Inconnue"),
            "text": row["text"],
            "distance": float(dist)
        })
    return results


def build_context(chunks):
    """Construit le contexte"""
    context_parts, total_len = [], 0
    for c in chunks:
        passage = f"[{c['source']}] {c['text']}"
        if total_len + len(passage) > MAX_CONTEXT_CHARS:
            break
        context_parts.append(passage)
        total_len += len(passage)
    return "\n\n---\n\n".join(context_parts)


def ask_openai(question, context, model="gpt-4o-mini", temperature=0.0):
    """Appel √† OpenAI"""
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": INSTRUCTION_PROMPT + "\n\nContexte :\n" + context},
        {"role": "user", "content": "Question : " + question},
    ]
    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=800
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"‚ö†Ô∏è Erreur OpenAI : {e}"

# =====================================================
# üîπ Cr√©ation de l‚Äôapplication FastAPI
# =====================================================
app = FastAPI(title="JuridiBot API", version="1.0", description="Assistant juridique marocain IA")

# üî∏ Autoriser les requ√™tes Flutter
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ou restreindre √† ton IP locale
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =====================================================
# üîπ Endpoint principal
# =====================================================
@app.get("/ask")
def ask(question: str = Query(..., description="Question juridique")):
    """R√©pond √† une question en se basant uniquement sur les PDFs index√©s"""
    print(f"‚ùì Question re√ßue : {question}")

    # √âtape 1 : R√©cup√©ration des passages
    chunks = retrieve(question, top_k=TOP_K)

    # √âtape 2 : Filtrage par distance
    relevant_chunks = [c for c in chunks if c["distance"] < DISTANCE_THRESHOLD]
    if not relevant_chunks:
        return {
            "answer": "Je ne peux pas r√©pondre √† cette question car elle ne figure pas dans ma base de connaissances juridiques.",
            "context_found": False,
            "sources": [],
        }

    # √âtape 3 : Construction du contexte
    context = build_context(relevant_chunks)

    # √âtape 4 : G√©n√©ration de r√©ponse
    answer = ask_openai(question, context)

    # √âtape 5 : Retourner la r√©ponse
    return {
        "answer": answer,
        "context_found": True,
        "sources": list({c["source"] for c in relevant_chunks}),
        "count_chunks": len(relevant_chunks),
    }

# =====================================================
# üîπ Lancer le serveur (pour test local)
# =====================================================
# Commande √† ex√©cuter :
# uvicorn src.api_juridibot:app --host 0.0.0.0 --port 8000 --reload